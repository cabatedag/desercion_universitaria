{"cells":[{"cell_type":"markdown","metadata":{"id":"E5W-65MskuXx"},"source":["# **Este *ipynb* está enmarcado dentro de un trabajo que busca predecir qué alumnos del departamento de Sistemas la UTN FRBA desertarán.**\n","\n","Los datos disponibles fueron unificados en una sola tabla y se removieron los registros con valores nulos.\n","\n","En este **ipynb** se utilizarán herramientas del aprendizaje estadístico para crear un modelo supervisado que prediga el outcome de la variable 'deserto'."]},{"cell_type":"markdown","metadata":{"id":"hWk9Vqs8l5jg"},"source":["## **Importación de librerías**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7VzMW3fvkuX0"},"outputs":[],"source":["## Importamos librerías para manipulación de datos.\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NFgDAUVLNfhE"},"outputs":[],"source":["## Importamos librerías de aprendizaje automático.\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.utils import shuffle\n","from imblearn.over_sampling import RandomOverSampler\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import FunctionTransformer\n","from sklearn.model_selection import GridSearchCV\n","from sklearn import svm\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","from sklearn.model_selection import cross_val_score\n","from sklearn.metrics import roc_curve, auc\n","from sklearn.metrics import confusion_matrix\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.datasets import make_classification\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ga5ltZUcMTtO"},"outputs":[],"source":["# Importamos librerías para poder crear Pipelines.\n","from sklearn.pipeline import Pipeline\n","from sklearn.impute import SimpleImputer\n","from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import OneHotEncoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4qczm8A0anNk"},"outputs":[],"source":["# Importamos librerias de PCA.\n","from sklearn.decomposition import PCA"]},{"cell_type":"markdown","metadata":{"id":"3SbYS6Q7qn4-"},"source":["## **Google Colaboratory o Local**\n","El notebook podrá ser corrido tanto localmente como en Google Colaboratory.\n","\n","El usuario deberá modificar el root path de acuerdo a su conveniencia."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-sMQ0Jxuo982"},"outputs":[],"source":["## Verificamos si estamos corriendo el noteboock en Google Colaboratory.\n","var_google_colab = 'google.colab' in str(get_ipython())\n","print(var_google_colab)\n","\n","## En el caso de estar en Google Colab, montamos nuestro Drive.\n","if var_google_colab:\n","  from google.colab import drive\n","  drive.mount('/content/gdrive',force_remount=True)\n","  ## Direccion root donde está el notebook.\n","  root_path = \"/content/gdrive/MyDrive/Colab Notebooks/GIAR/\"\n","\n","## En el caso de no estar en Google Colab, estamos corriendo localmente el notebook.\n","else:\n","  root_path = \"\""]},{"cell_type":"markdown","metadata":{"id":"cd4eSEt9vaji"},"source":["## **Dataset**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"luLmTrKqeGhw"},"outputs":[],"source":["## Importamos el dataset.\n","df = pd.read_csv(root_path + 'datos/base_datos_estudiantes02_00.csv')"]},{"cell_type":"markdown","metadata":{"id":"v7h4SOeAkuX4"},"source":["## **PREPOCESAMIENTO**"]},{"cell_type":"code","source":["df.columns"],"metadata":{"id":"7qXPVWKMx5UO"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yrzugvtutpaQ"},"outputs":[],"source":["## Imprimimos las dimensiones del dataset.\n","print(f'Dimensiones del dataset: {df.shape}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mQViqbcuF8I1"},"outputs":[],"source":["## Calculamos el Sample to Feature Ratio (S2FR) y lo imprimimos.\n","S2FR = df.shape[0] / (df.shape[1]-2)\n","S2FR"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3izrpk3IHL2d"},"outputs":[],"source":["## Dividimos el dataset en la variable dependiente \"y\", en este caso \"deserto\", y las independientes \"x\".\n","y = np.array(df[[\"deserto\"]])\n","x = df.drop(['Codigo Alumno','deserto'], axis=1)"]},{"cell_type":"code","source":["x.shape"],"metadata":{"id":"dzeHE4J9y8M8"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ah4EIan3pzSB"},"outputs":[],"source":["## Diferenciamos las variables numéricas de las categóricas.\n","num_features = ['edad al ingreso',\n","                'Ciclo Lectivo de Cursada',\n","                'Cantidad de veces recursada regular',\n","                'Descripción de recursada regular_No Recurso', \n","                'Descripción de recursada regular_Recurso 1 Vez', \n","                'Descripción de recursada regular_Recurso 2 Veces', \n","                'Descripción de recursada regular_Recurso 3 Veces', \n","                'Descripción de recursada regular_Recurso 4 Veces', \n","                'Descripción de recursada regular_Recurso 5 Veces',\n","                'noAprobado', \n","                'Aprobado', \n","                'Promociono', \n","                'Nota', \n","                'Nota_max_prom', \n","                'Indice_aprobacion', \n","                'Turno_Mañana', \n","                'Turno_Noche', \n","                'Turno_Tarde', \n","                'Tipo de aprobación_Cambio Curso', \n","                'Tipo de aprobación_Firmo', \n","                'Tipo de aprobación_Libre', \n","                'Tipo de aprobación_No Firmo', \n","                'Tipo de aprobación_Promociono',\n","                'cantidad de años']\n","\n","cat_features = ['EsTecnico', 'Sexo','grupo_ingreso_nivel1']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RQqFB4q-apEg"},"outputs":[],"source":["## Separamos el dataset en train y test. \n","xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=824, random_state=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kice3-TeDIJg"},"outputs":[],"source":["xtrain.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vAU3573lENgv"},"outputs":[],"source":["xtest.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iGgYvqb9GYgM"},"outputs":[],"source":["pd.DataFrame(ytrain).value_counts(normalize = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cH8k_x7LFTy6"},"outputs":[],"source":["pd.DataFrame(ytest).value_counts(normalize = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LMENQTlRG71M"},"outputs":[],"source":["## Utilizamos la técnica de oversampling para balancear los datos de train.\n","oversample = RandomOverSampler(sampling_strategy='minority')\n","# fit and apply the transform\n","xtrain_over, ytrain_over = oversample.fit_resample(xtrain, ytrain)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i6UC4WAbIvrY"},"outputs":[],"source":["xtrain_over.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2UBb6FgZG92Z"},"outputs":[],"source":["pd.DataFrame(ytrain_over).value_counts(normalize=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1l-JxtNzOQmG"},"outputs":[],"source":["xtrain_over.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SeK_5TMzIHBd"},"outputs":[],"source":["## Definimos las transformaciones a realizar para las variables numéricas. \n","## Las estandarizaremos utilizando StandardScaler.\n","numeric_transformer = Pipeline(\n","    steps=[(\"scaler\", StandardScaler())]\n",")\n","\n","## Definimos la transformación a realizar para las variables categóricas.\n","do_nothing = FunctionTransformer(lambda x: x)\n","\n","## Definimos el ColumnTransformer que será ejecutado al momento del fitting.\n","preprocesamiento = ColumnTransformer(\n","    transformers=[ \n","        (\"num\", numeric_transformer, num_features),\n","        (\"cat\", do_nothing, cat_features)\n","    ]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZVREuGrQKxtK"},"outputs":[],"source":["## Matriz X al aplicar el preprosesamiento definido.\n","normalize_xtrain_over = pd.DataFrame(preprocesamiento.fit_transform(xtrain_over))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NlXcPtSIMDlm"},"outputs":[],"source":["normalize_train_over = pd.concat([pd.DataFrame(ytrain_over),normalize_xtrain_over], axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SU31rMZaMsGI"},"outputs":[],"source":["normalize_xtrain_over.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rb2NbiVPODL7"},"outputs":[],"source":["normalize_train_over.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wGzGkBwJLL8p"},"outputs":[],"source":["## Calculamos la matriz de correlación lineal de Pearson.\n","corr = normalize_train_over.corr()\n","# Imprimimos el heatmap.\n","fig = plt.figure(figsize=(20,20), dpi = 360)\n","sns.heatmap(corr, annot = True, fmt = '.2f')"]},{"cell_type":"markdown","metadata":{"id":"aIptcKRefEiS"},"source":["##**Pipeline de Machine Learning**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BVuOwEAeino7"},"outputs":[],"source":["## Creamos un Pipeline para aplicar secuencialmente la lista de transformaciones definida previamente y un estimador final.\n","pipeline1 = Pipeline(\n","    steps=[(\"preprocesamiento\",preprocesamiento),(\"estimador\",SVC(probability=True))]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A7BAg_NcioUG"},"outputs":[],"source":["## Definimos los hiperparámetros de los modelos a comparar.\n","parametros = [\n","    {    \n","        \"estimador\": (LogisticRegression(),),\n","          \"estimador__C\": (0.001, 1, 10, 100)\n","    },\n","     {\n","        \"estimador\": (SVC(probability=True),),\n","          \"estimador__kernel\":('rbf',), \n","          \"estimador__C\":(0.1, 1, 10, 100, 1000), \n","          \"estimador__gamma\":(0.001, 0.0001)      \n","    }\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-e_3HU7PoEv4"},"outputs":[],"source":["## Creamos el Grid Search + Cross Validation.\n","grid_search1 = GridSearchCV(pipeline1, parametros,\n","                  refit = True,\n","                   cv = 5,\n","                   verbose=40)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0v56ZpEuoKcq"},"outputs":[],"source":["## Implementamos el GSCV con el set de train.\n","grid_search1.fit(xtrain_over,ytrain_over.ravel())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QHR-IbwNoUHV"},"outputs":[],"source":["## Guardamos en una variable toda la informacion del entrenamiento que quedó registrada en 'cv_results_'.\n","scores = grid_search1.cv_results_\n","\n","## Imprimimos la informacion del entrenamiento.\n","scores_df = pd.DataFrame.from_dict(scores)\n","scores_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vOBPzUa6up4u"},"outputs":[],"source":["## Imprimimos cual fue la mejor combinación de hiperparámetros.\n","print(\"Con un score de %0.4f, la mejor combinación de hiperparámtros fue: \\n %s\" % (grid_search1.best_score_, grid_search1.best_params_))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xjDReIWBurU-"},"outputs":[],"source":["## Obtenemos las predicciones a partir del modelo entrenado.\n","ypred = grid_search1.predict(xtest)"]},{"cell_type":"markdown","metadata":{"id":"_nkYoqdkaMG1"},"source":["##**Mediciones de desempeño**##"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BsLmpTQduu3Z"},"outputs":[],"source":["## Calculamos el accuracy.\n","test_acc = accuracy_score(ytest, ypred)\n","print(\"El accuracy es \" + str(test_acc))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AFDZz3X3u6M6"},"outputs":[],"source":["## Computamos el área abajo de la curva ROC.\n","yproba = grid_search1.predict_proba(xtest)\n","fpr, tpr, thresholds = roc_curve(ytest.astype(int), yproba[:,1], drop_intermediate = False, pos_label=1)\n","auc_value = auc(fpr, tpr)\n","print(\"El AUC es = \" + str(auc_value))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4mxYiNFlvAMU"},"outputs":[],"source":["## Imprimimos la curva ROC.\n","plt.figure(figsize=(7,7))\n","plt.plot(fpr, tpr, lw=2, alpha=0.7 , label = 'Curva ROC', color = 'b')\n","plt.plot([0, 1], [0, 1], linestyle='--', lw=1, color='r', alpha=.8)\n","plt.xlabel('False Positive Rate', fontsize=13)\n","plt.ylabel('True Positive Rate', fontsize=13)\n","plt.grid(False)\n","plt.legend(loc=\"lower right\",fontsize=12)\n","plt.title('Curva ROC',fontsize=18)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a-Wcm6DwvBhi"},"outputs":[],"source":["## Calculamos e imprimimos la Matriz de Confusión.\n","cm = confusion_matrix(ytest, ypred)\n","df_cm = pd.DataFrame(cm, index = ['No accede', 'Accede'], columns = ['No accede', \"Accede\"])\n","plt.figure(figsize = (10,7))\n","sns.heatmap(df_cm, annot=True,fmt='g')\n","plt.title('Matriz de Confusión', fontsize=16)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"e4KAPXvw-T9O"},"source":["##**Analisis de Componentes Pricipales**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hsYt6P4RDFJc"},"outputs":[],"source":["## Fijamos el porcentaje de varianza que deberán explicar los autovectores que se seleccionarán. \n","n_comps = 0.8\n","\n","## Definimos el PCA.\n","pca = PCA(n_components= n_comps)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3T-kjJVx-W5i"},"outputs":[],"source":["## Definimos las transformaciones a realizar para las variables numéricas. \n","## Las estandarizaremos utilizando StandardScaler y luego aplicamos el método PCA.\n","numeric_transformer2 = Pipeline(\n","    steps=[(\"scaler\", StandardScaler()), (\"pca\", pca)]\n",")\n","\n","## Juntamos las transformaciones definidas previamente.\n","## Definimos el ColumnTransformer que será ejecutado al momento del fitting.\n","preprocesamiento2 = ColumnTransformer(\n","    transformers=[ \n","        (\"num\", numeric_transformer2, num_features),\n","        (\"cat\", do_nothing, cat_features)\n","    ]\n",")\n","\n","## Definimos un nuevo pipeline que incluye el método PCA en el preprosemiento.\n","pipeline2=Pipeline(\n","    steps=[('preprocesamiento',preprocesamiento2),('estimador',SVC())]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g-L8iQBKcu5-"},"outputs":[],"source":["## Observamos como queda el dataset que se obtine a partir de la reducción de la dimensionalidad.\n","pd.DataFrame(preprocesamiento2.fit_transform(xtrain_over))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aPZUH7GoW4z1"},"outputs":[],"source":["## Creamos el nuevo GSCV.\n","grid_search2 = GridSearchCV(pipeline2, parametros,\n","                  refit = True, \n","                   cv = 5,\n","                   verbose=40)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iG8iN_49ACi7"},"outputs":[],"source":["## Implementamos el GSCV.\n","grid_search2.fit(xtrain_over,ytrain_over.ravel())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GpHHN3irB0ul"},"outputs":[],"source":["## Guardamos en una variable toda la informacion del entrenamiento que quedó registrada en 'cv_results_'.\n","scores2 = grid_search2.cv_results_\n","\n","## Imprimimos la informacion del entrenamiento.\n","scores2_df = pd.DataFrame.from_dict(scores)\n","scores2_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vxhKhDENA1J2"},"outputs":[],"source":["## Imprimimos cual fue el mejor modelo (estimator) y combinación de hiperparámetros.\n","print(\"Con un score de %0.4f, la mejor combinación de hiperparámtros fue: \\n %s\" % (grid_search2.best_score_, grid_search2.best_params_))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KmAumWArCA01"},"outputs":[],"source":["## Obtenemos las predicciones a partir del modelo entrenado.\n","ypred2 = grid_search2.predict(xtest)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8voFX5InCBnd"},"outputs":[],"source":["## Calculamos el accuracy.\n","test_acc = accuracy_score(ytest, ypred2)\n","print(\"El accuracy es \" + str(test_acc))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jYJv0JSUCB44"},"outputs":[],"source":["## Computamos el área abajo de la curva ROC.\n","yproba2 = grid_search2.predict_proba(xtest)\n","fpr2, tpr2, thresholds = roc_curve(ytest.astype(int), yproba2[:,1], drop_intermediate = False, pos_label=1)\n","auc_value = auc(fpr2, tpr2)\n","print(\"El AUC es = \" + str(auc_value))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G_E-8GkCCCAc"},"outputs":[],"source":["## Imprimimos la curva ROC.\n","plt.figure(figsize=(7,7))\n","plt.plot(fpr2, tpr2, lw=2, alpha=0.7 , label = 'Curva ROC', color = 'b')\n","plt.plot([0, 1], [0, 1], linestyle='--', lw=1, color='r', alpha=.8)\n","plt.xlabel('False Positive Rate', fontsize=13)\n","plt.ylabel('True Positive Rate', fontsize=13)\n","plt.grid(False)\n","plt.legend(loc=\"lower right\", fontsize=12)\n","plt.title('Curva ROC', fontsize=16)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0GjrMjk1CCE-"},"outputs":[],"source":["## Calculamos e imprimimos la Matriz de Confusión.\n","cm = confusion_matrix(ytest, ypred2)\n","df_cm = pd.DataFrame(cm, index = ['No accede', 'Accede'], columns = ['No accede', \"Accede\"])\n","plt.figure(figsize = (10,7))\n","sns.heatmap(df_cm, annot=True,fmt='g')\n","plt.title('Matriz de Confusión', fontsize=16)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-78EeKx-cqML"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nr1msehWfOc1"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1fWFK9ZKDqL7KcWIYVwb7M5E6RZb7Gg0y","timestamp":1668900795788}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"vscode":{"interpreter":{"hash":"c63d8c7d738c2960218a10995aedf0a7f67a49a231e71037adf0440953cdb45b"}}},"nbformat":4,"nbformat_minor":0}